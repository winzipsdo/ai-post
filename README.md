<table>
  <thead>
    <tr>
      <th>UTC Date</th>
      <th>Resource</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>17/02/2025</td>
      <td><a href="https://arxiv.org/abs/2502.12115">SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</a></td>
      <td>SWE-Lancer is a benchmark of 1,400 freelance software engineering tasks worth $1 million. It includes both independent and managerial tasks, with performance evaluation by experts. Frontier models struggle with most tasks. The benchmark is open-sourced to advance research on AI's economic impact.</td>
    </tr>
    <tr>
      <td>12/02/2025</td>
      <td>
        <a
          href="https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/"
          >OmniParser V2: Turning Any LLM into a Computer Use Agent</a
        >
      </td>
      <td>
        OmniParser V2 improves GUI automation by converting UI screenshots into structured elements for LLMs, achieving
        60% faster inference and higher accuracy. It scores 39.6 on the ScreenSpot Pro benchmark, a major improvement
        over GPT-4o's 0.8.
      </td>
    </tr>
    <tr>
      <td>22/01/2025</td>
      <td>
        <a href="https://arxiv.org/abs/2501.12948"
          >DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a
        >
      </td>
      <td>
        We introduce two reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, trained via large-scale
        reinforcement learning (RL) without supervised fine-tuning, shows strong reasoning capabilities but faces issues
        like poor readability. To improve this, DeepSeek-R1 incorporates multi-stage training and cold-start data,
        achieving performance comparable to OpenAI's models. We open-source both models and six distilled versions based
        on Qwen and Llama.
      </td>
    </tr>
    <tr>
      <td>12/06/2017</td>
      <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></td>
      <td>
        The Transformer is a new attention-based architecture that outperforms traditional models in machine translation
        and other tasks, with faster training and state-of-the-art results.
      </td>
    </tr>
  </tbody>
</table>
