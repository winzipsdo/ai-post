<table>
  <thead>
    <tr>
      <th>UTC Date</th>
      <th>Resource</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>12/06/2017</td>
      <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></td>
      <td>The Transformer is a new attention-based architecture that outperforms traditional models in machine translation and other tasks, with faster training and state-of-the-art results.</td>
    </tr>
  </tbody>
</table>
