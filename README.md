<table>
  <thead>
    <tr>
      <th>UTC Date</th>
      <th>Resource</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>12/02/2025</td>
      <td>
        <a
          href="https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/"
          >OmniParser V2: Turning Any LLM into a Computer Use Agent</a
        >
      </td>
      <td>
        OmniParser V2 improves GUI automation by converting UI screenshots into structured elements for LLMs, achieving
        60% faster inference and higher accuracy. It scores 39.6 on the ScreenSpot Pro benchmark, a major improvement
        over GPT-4o's 0.8.
      </td>
    </tr>
    <tr>
      <td>12/06/2017</td>
      <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></td>
      <td>
        The Transformer is a new attention-based architecture that outperforms traditional models in machine translation
        and other tasks, with faster training and state-of-the-art results.
      </td>
    </tr>
  </tbody>
</table>
